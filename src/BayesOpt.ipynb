{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalised Forest Tuning - Bayesian Optimisation\n",
    "\n",
    "This is the code used to run experiments on Bayesian optimisation in the paper \"Generalising Random Forest Parameter Optimisation to Include Stability and Cost\" by CHB Liu, BP Chamberlain, DA Little, A Cardoso (2017).\n",
    "\n",
    "Please ensure you are using the anaconda environment `gft_env`. This is usually indicated by successfully importing the libraries below. If the library import resulted in any error, please try and run the `./setup_environment.sh` script again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd # You need pandas 0.19+\n",
    "import numpy as np\n",
    "\n",
    "from data_loader import *\n",
    "from evaluator import *\n",
    "from pybo import solve_bayesopt\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimisation Wrapper Function\n",
    "\n",
    "The following function takes the training and validation features and labels, the loss function weights ($\\alpha$, $\\beta$, and $\\gamma$), and returns the best forest parameter combination, the Bayesian optimisation model, intermediary results, and the metrics achieved with the best forest parameter combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bayesopt_RF_performace(\n",
    "    X_train, X_val, y_train, y_val,\n",
    "    weight_alpha=1, weight_beta=1, weight_gamma=1):\n",
    "\n",
    "    objective = \\\n",
    "        partial(get_RF_generalised_performance_score,\n",
    "                features_train_complete=X_train,\n",
    "                features_val=X_val,\n",
    "                labels_train_complete=y_train,\n",
    "                labels_val=y_val,\n",
    "                weight_alpha=weight_alpha,\n",
    "                weight_beta=weight_beta,\n",
    "                weight_gamma=weight_gamma,\n",
    "                verbose=False)\n",
    "\n",
    "    xbest, model, info = solve_bayesopt(\n",
    "        objective,\n",
    "        bounds=[[5, 200], [1, 20], [0.1, 1]],\n",
    "        niter=20, \n",
    "        verbose=True)\n",
    "\n",
    "    best_performance = \\\n",
    "         train_and_get_RF_performance(\n",
    "             np.array(xbest), \n",
    "             X_train, \n",
    "             X_val,\n",
    "             y_train, \n",
    "             y_val, \n",
    "             nrun=10)\n",
    "        \n",
    "    return xbest, model, info, best_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian optimisation for Orange small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "orange_small_features, _, _, orange_small_upselling_labels = \\\n",
    "    get_and_process_orange_small_data()\n",
    "\n",
    "# Do train validation split:\n",
    "# We select the first <prop> data as train data, \n",
    "# the rest will be the hold out validation set\n",
    "orange_small_features_train, orange_small_features_val, \\\n",
    "orange_small_upselling_labels_train, orange_small_upselling_labels_val = \\\n",
    "    split_train_val_data(orange_small_features,\n",
    "                         orange_small_upselling_labels,\n",
    "                         prop=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To observe the forest parameters' sensitivity on weight parameters,\n",
    "# change the values of <weight_alpha>, <weight_beta> and <weight_gamma>\n",
    "best_param, _, _, best_performance = \\\n",
    "    bayesopt_RF_performace(\n",
    "        orange_small_features_train, orange_small_features_val,\n",
    "        orange_small_upselling_labels_train, \n",
    "        orange_small_upselling_labels_val,\n",
    "        weight_alpha=1, weight_beta=1, weight_gamma=0.01\n",
    "    )\n",
    "\n",
    "# Print results\n",
    "print(\"\")\n",
    "print(\"------ BAYESIAN OPTIMISATION RESULT ------\")\n",
    "print(\"The best parameter combination (with the highest \"\n",
    "      \"posterior mean) found by Bayesian optimisation is:\")\n",
    "print(\"No. trees: \" + str(int(best_param[0])) + \n",
    "      \", Max tree depth: \" + str(int(best_param[1])) +\n",
    "      \", Training prop.: \" + str(best_param[2]) + \"\\n\")\n",
    "\n",
    "print(\"The metrics achieved under this parameter combination \"\n",
    "      \"is as follow:\")\n",
    "print(best_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bayesian optimisation for Criteo dataset\n",
    "\n",
    "Warning: Long running process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Default to train and tune parameters with 10% of criteo data to\n",
    "# prevent memory issues\n",
    "# If you have >64GB of memory and would like to use the full\n",
    "# dataset, set the <sample> argument to False\n",
    "criteo_features, criteo_labels = \\\n",
    "    get_and_process_criteo_data(sample=True)\n",
    "\n",
    "# Do train validation split:\n",
    "# We select the first <prop> data as train data, \n",
    "# the rest will be the hold out validation set\n",
    "criteo_features_train, criteo_features_val, \\\n",
    "criteo_labels_train, criteo_labels_val = \\\n",
    "    split_train_val_data(criteo_features,\n",
    "                         criteo_labels,\n",
    "                         prop=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To observe the forest parameters' sensitivity on weight parameters,\n",
    "# change the values of <weight_alpha>, <weight_beta> and <weight_gamma>\n",
    "best_param, _, _, best_performance = \\\n",
    "    bayesopt_RF_performace(\n",
    "        criteo_features_train, criteo_features_val,\n",
    "        criteo_labels_train, criteo_labels_val,\n",
    "        weight_alpha=1, weight_beta=1, weight_gamma=0.001\n",
    "    )\n",
    "\n",
    "# Print results\n",
    "print(\"\")\n",
    "print(\"------ BAYESIAN OPTIMISATION RESULT ------\")\n",
    "print(\"The best parameter combination (with the highest \"\n",
    "      \"posterior mean) found by Bayesian optimisation is:\")\n",
    "print(\"No. trees: \" + str(int(best_param[0])) + \n",
    "      \", Max tree depth: \" + str(int(best_param[1])) +\n",
    "      \", Training prop.: \" + str(best_param[2]) + \"\\n\")\n",
    "\n",
    "print(\"The metrics achieved under this parameter combination \"\n",
    "      \"is as follow:\")\n",
    "print(best_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:gft_env]",
   "language": "python",
   "name": "conda-env-gft_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
